{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rating some movies\n",
    "#### To make recommendation for you, we are going to learn your taste by asking you to rate a few movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks like you've already rated the movies. Overwrite ratings (y/N)? n\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from time import time\n",
    "from os.path import join, isfile, dirname\n",
    "\n",
    "topMovies = \"\"\"1,Toy Story (1995)\n",
    "780,Independence Day (a.k.a. ID4) (1996)\n",
    "590,Dances with Wolves (1990)\n",
    "1210,Star Wars: Episode VI - Return of the Jedi (1983)\n",
    "648,Mission: Impossible (1996)\n",
    "344,Ace Ventura: Pet Detective (1994)\n",
    "165,Die Hard: With a Vengeance (1995)\n",
    "153,Batman Forever (1995)\n",
    "597,Pretty Woman (1990)\n",
    "1580,Men in Black (1997)\n",
    "231,Dumb & Dumber (1994)\"\"\"\n",
    "\n",
    "parentDir = os.path.abspath('')\n",
    "ratingsFile = join(parentDir, \"personalRatings.txt\")\n",
    "\n",
    "if isfile(ratingsFile):\n",
    "    r = input(\"Looks like you've already rated the movies. Overwrite ratings (y/N)? \")\n",
    "    if r and r[0].lower() == \"y\":\n",
    "        os.remove(ratingsFile)\n",
    "    else:\n",
    "        sys.exit()\n",
    "\n",
    "prompt = \"Please rate the following movie (1-5 (best), or 0 if not seen): \"\n",
    "print(prompt)\n",
    "\n",
    "now = int(time())\n",
    "n = 0\n",
    "\n",
    "f = open(ratingsFile, 'w')\n",
    "for line in topMovies.split(\"\\n\"):\n",
    "    ls = line.strip().split(\",\")\n",
    "    valid = False\n",
    "    while not valid:\n",
    "        rStr = input(ls[1] + \": \")\n",
    "        r = int(rStr) if rStr.isdigit() else -1\n",
    "        if r < 0 or r > 5:\n",
    "            print(prompt)\n",
    "        else:\n",
    "            valid = True\n",
    "            if r > 0:\n",
    "                f.write(\"0::%s::%d::%d\\n\" % (ls[0], r, now))\n",
    "                n += 1\n",
    "f.close()\n",
    "\n",
    "if n == 0:\n",
    "    print(\"No rating provided!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from operator import add\n",
    "from os.path import join, isfile, dirname\n",
    "\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from pyspark.sql.functions import col, desc\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "def parseMovie(line):\n",
    "    \"\"\"\n",
    "    Parses a movie record in MovieLens format movieId::movieTitle .\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"::\")\n",
    "    return int(fields[0]), fields[1], fields[2]\n",
    "\n",
    "def parseRating(line):\n",
    "    \"\"\"\n",
    "    Parses a rating record in MovieLens format userId::movieId::rating::timestamp .\n",
    "    \"\"\"\n",
    "    fields = line.strip().split(\"::\")\n",
    "    return int(fields[3]) % 10, (int(fields[0]), int(fields[1]), float(fields[2]))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # set up environment\n",
    "    spark = SparkSession.builder \\\n",
    "   .master(\"local\") \\\n",
    "   .appName(\"Movie Recommendation Engine\") \\\n",
    "   .config(\"spark.executor.memory\", \"1gb\") \\\n",
    "   .getOrCreate()\n",
    "   \n",
    "    sc = spark.sparkContext\n",
    "\n",
    "    # personal ratings\n",
    "    myRatings = sc.textFile(os.path.abspath('./personalRatings.txt')).map(parseRating) \n",
    "    myRatingsDF = myRatings.map(lambda line: Row(userID=line[1][0], movieID=line[1][1], \n",
    "                                                 rating=line[1][2], timestamp=line[0])).toDF()\n",
    "    \n",
    "    # load ratings and movie titles\n",
    "    movieLensHomeDir = os.path.abspath('.')\n",
    "    \n",
    "    # movies is an RDD of (movieId, movieTitle)\n",
    "    movies = sc.textFile(join(movieLensHomeDir, \"movies.dat\")).map(parseMovie)\n",
    "    moviesDF = movies.map(lambda line: Row(movieID=line[0], movieTitle=line[1], genre=line[2])).toDF()\n",
    "    # moviesDF.describe() -- example of how I explored the Data\n",
    "    \n",
    "    # ratings is an RDD of (last digit of timestamp, (userId, movieId, rating))\n",
    "    ratings = sc.textFile(join(movieLensHomeDir, \"ratings.dat\")).map(parseRating)\n",
    "    ratingsDF = ratings.map(lambda line: Row(userID=line[1][0], movieID=line[1][1], rating=line[1][2],\n",
    "                            timestamp=line[0])).toDF()\n",
    "    \n",
    "    #join the ratings DF with my ratings to get the full set of ratings\n",
    "    combinedRatings = ratingsDF.union(myRatingsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "    \"\"\"\n",
    "    This code block deals with the standardisation of the ratings data.\n",
    "    I use a standard scaler from the pyspark ml library\n",
    "    The output of the scaling will go into the rating_scaled column as a DenseVector\n",
    "    \"\"\"\n",
    "    \n",
    "    df = combinedRatings.rdd.map(lambda line: Row(userID=line[0], movieID=line[1], \n",
    "                                                          rating=DenseVector(line[2:]), timestamp=line[3])).toDF()\n",
    "\n",
    "    standardScaler = StandardScaler(inputCol=\"rating\", outputCol=\"rating_scaled\")\n",
    "    scaler = standardScaler.fit(df)\n",
    "    scaledRatingsDF = scaler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 97:>                                                         (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "    #format the DF so that the ratings column is out of the Dense Vector\n",
    "    standardisedRatingsDF = scaledRatingsDF.rdd.map(lambda line: Row(userID=line[0], movieID=line[1], \n",
    "                                                          rating=float(line[4][0]), timestamp=line[3])).toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE:  0.6221704222725032\n"
     ]
    }
   ],
   "source": [
    "    '''\n",
    "    The following lines define the parameters for the ALS\n",
    "    I found that max iterations 10 is the best so that the computation time is short\n",
    "    The reg param is set to 0.1 because a big number would cause inaccurate results\n",
    "    whereas a number that is too small would make the als never converge. 0.1 works well.\n",
    "    I experimented with different ranks, 6 worked the best - gave me the best MSE\n",
    "    Nonnegative is true because all the ratings are not negative\n",
    "    Implicit prefs is false because we are working with explicit data\n",
    "    '''\n",
    "    als = ALS(\n",
    "            maxIter=10,\n",
    "            regParam=0.1,\n",
    "            rank=6,\n",
    "            userCol=\"userID\",\n",
    "            itemCol = \"movieID\",\n",
    "            ratingCol = \"rating\",\n",
    "            nonnegative = True,\n",
    "            implicitPrefs = False,\n",
    "            coldStartStrategy = \"drop\"\n",
    "    )\n",
    "    \n",
    "    #split the full list of ratings into a training set and test set\n",
    "    #The training set is 80% of the data, the test  set is 20%\n",
    "    (trainDF, testDF) = standardisedRatingsDF.randomSplit([0.8, 0.2])\n",
    "\n",
    "    #fit the als to the training set\n",
    "    model = als.fit(trainDF)\n",
    "    \n",
    "    #this is the prediction DataFrame made with the als model perfomed on the training set\n",
    "    predictionDF = model.transform(testDF)\n",
    "    \n",
    "    evaluate = RegressionEvaluator(metricName=\"mse\", labelCol=\"rating\",  predictionCol=\"prediction\")\n",
    "    MSE = evaluate.evaluate(predictionDF)\n",
    "    print('MSE: ', MSE)\n",
    "\n",
    "    #get only the ratings with the userID: 0\n",
    "    myUser = combinedRatings.filter(combinedRatings['userID'] == 0)\n",
    "    \n",
    "    #recommend 5 movies for just the one user - saves on computation time!\n",
    "    recommendMoviesDF = model.recommendForUserSubset(myUser, 5)\n",
    "    \n",
    "    #select the recommendations column\n",
    "    myRecommendedMovies = recommendMoviesDF.select(\"recommendations\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "    \n",
    "    movieIDs = []\n",
    "    \n",
    "    #loop through the 'recommendations' column in recommended movies\n",
    "    #and apped the tuple: movieID, rating to the list of movieIDs\n",
    "    for movie in myRecommendedMovies.first()['recommendations']:\n",
    "        movieData = (movie['movieID'], movie['rating'])\n",
    "        movieIDs.append(movieData)\n",
    "    \n",
    "    #DF of my recommended movies\n",
    "    myMoviesDF = spark.createDataFrame(data=movieIDs, schema = [\"movieID\", \"rating\"])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    #join the movies DataFrame with my recommended movies\n",
    "    moviesJoined = myMoviesDF.join(moviesDF, on=\"movieID\", how=\"left\")\n",
    "    \n",
    "    #order the joined DF by rating descending\n",
    "    moviesJoined = moviesJoined.orderBy(col(\"rating\").desc())\n",
    "    \n",
    "    #extract just the movie title from the resulting DF\n",
    "    moviesText = moviesJoined.select(\"movieTitle\").rdd.flatMap(list).collect()\n",
    "    \n",
    "    #print recommended movies\n",
    "    print(\"Movies recommended for you:\")\n",
    "    for index, movie in enumerate(moviesText):\n",
    "        print(index+1, \": \", movie)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # clean up\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
